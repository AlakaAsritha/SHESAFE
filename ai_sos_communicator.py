# -*- coding: utf-8 -*-
"""AI_SOS_COMMUNICATOR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rFt3n1u55NJRZYHDVp0mfqulxK0HEYcM

## SPEECH STRESS DETECTION
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("uwrfkaggler/ravdess-emotional-speech-audio")

print("Path to dataset files:", path)

"""## Training an LSTM model on the RAVDESS dataset for speech stress detection.

This code trains an LSTM (Long Short-Term Memory) model to recognize emotions from speech using the RAVDESS dataset. It extracts MFCC (Mel-Frequency Cepstral Coefficients) as features from audio files and classifies speech into one of 8 emotion categories.
"""

import tensorflow as tf
import numpy as np
import librosa
import os
import glob
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

# Load RAVDESS dataset
def load_ravdess_dataset(dataset_path):
    audio_data = []
    labels = []

    for file in glob.glob(os.path.join(dataset_path, "**/*.wav"), recursive=True):
        filename = os.path.basename(file)  # Extract only the filename
        parts = filename.split("-")

        # Debugging
        print(f"Processing file: {filename}, Split parts: {parts}")

        try:
            label = int(parts[2]) - 1  # Extract the emotion label (1-8) and convert to 0-7
        except ValueError:
            print(f"Skipping file: {filename} (invalid format)")
            continue

        signal, sr = librosa.load(file, sr=22050)
        mfcc = extract_mfcc(signal, sr)
        audio_data.append(mfcc)
        labels.append(label)

    return np.array(audio_data), np.array(labels)

# Extract MFCC features with normalization
def extract_mfcc(audio, sr=22050):
    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)
    scaler = MinMaxScaler()
    mfccs_scaled = scaler.fit_transform(mfccs)  # Normalize MFCC
    return np.mean(mfccs_scaled.T, axis=0)  # Average across time axis

# Prepare dataset
def prepare_data(dataset_path):
    audio_data, labels = load_ravdess_dataset(dataset_path)
    X_train, X_test, y_train, y_test = train_test_split(audio_data, labels, test_size=0.2, random_state=42)

    # Reshape for LSTM: (samples, timesteps=1, features=13)
    X_train = np.expand_dims(X_train, axis=1)
    X_test = np.expand_dims(X_test, axis=1)

    return X_train, X_test, y_train, y_test

# Train LSTM model
def train_lstm_model(X_train, y_train, X_test, y_test):
    model = Sequential([
        LSTM(128, return_sequences=True, input_shape=(1, 13)),  # Fixed shape
        Dropout(0.2),
        LSTM(64),
        Dropout(0.2),
        Dense(8, activation='softmax')  # 8 emotion classes
    ])

    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=32)
    return model

# Save trained model
def save_model(model, model_path):
    model.save(model_path)

# Main execution
def main():
    dataset_path = "/root/.cache/kagglehub/datasets/uwrfkaggler/ravdess-emotional-speech-audio/versions/1"
    X_train, X_test, y_train, y_test = prepare_data(dataset_path)
    model = train_lstm_model(X_train, y_train, X_test, y_test)
    save_model(model, "speech_stress_model.keras")  # Use .keras format
    print("âœ… Model training complete and saved!")

if __name__ == "__main__":
    main()

""" ## Convert Model to TensorFlow Lite (TFLite)"""

import tensorflow as tf

# Load the trained Keras model
model = tf.keras.models.load_model("speech_stress_model.h5")

# Recompile the model (optional)
model.compile(loss="sparse_categorical_crossentropy", optimizer="adam", metrics=["accuracy"])

print("Model loaded and compiled successfully!")

"""This code is designed for Speech Emotion Recognition (SER) using the RAVDESS dataset. It processes audio files, extracts features, trains an LSTM-based deep learning model, and saves it for later inference.

Converting a TensorFlow (Keras) model to TensorFlow Lite (TFLite) makes the model optimized for deployment on edge devices, such as:
1. Mobile Phones (Android, iOS)
2. Embedded Systems (Raspberry Pi, Jetson Nano)
3. IoT Devices (ESP32, microcontrollers)
"""

import tensorflow as tf

# Load the trained model
model = tf.keras.models.load_model("speech_stress_model.keras")

# Create a TFLite converter
converter = tf.lite.TFLiteConverter.from_keras_model(model)

# Enable resource variables (fix TensorListReserve issue)
converter.experimental_enable_resource_variables = True

# Allow TensorFlow ops for unsupported TFLite layers
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,  # Default TFLite ops
    tf.lite.OpsSet.SELECT_TF_OPS     # Allow TensorFlow ops (for LSTM)
]

# Fix tensor list conversion issue
converter._experimental_lower_tensor_list_ops = False

# Convert the model
tflite_model = converter.convert()

# Save the converted model
with open("speech_stress_model.tflite", "wb") as f:
    f.write(tflite_model)

print("âœ… Model successfully converted to TensorFlow Lite!")

"""## MOTION ANOMALY DETECTION

##DATA COLLECTION AND PREPROCESSING

Load MotionSense dataset and check available files.




Extract accelerometer & gyroscope data.

Normalize data using MinMaxScaler.

Create sliding windows (100 samples per window).

Split train-test (70% for training).

Train LSTM Autoencoder (for normal motion).

Train One-Class SVM (for anomaly detection).

Convert LSTM model to TensorFlow Lite (for mobile).

**MOBI ACT DATASET**

 Includes accelerometer and gyroscope data from smartphones.
Contains fall detection scenarios, which are useful for identifying distress situations.
Activities include walking, running, jumping, sitting, standing, and simulated falls (forward, backward, sideward, etc.).


Why it fits OUR project?

Best for detecting emergency situations, like sudden falls or rapid movement changes.
Can help train the model to distinguish between normal and distress movements.

MobiAct dataset files contain motion data from phone sensors, including acceleration (BSC_acc), gyroscope (BSC_gyro), and orientation (BSC_ori). Each file follows a structured format:

Acceleration Data (BSC_acc_x_x.txt):

Columns: timestamp (ns), x, y, z (m/s^2)
Represents acceleration forces including gravity along x, y, and z axesâ€‹
â€‹
â€‹
.
Gyroscope Data (BSC_gyro_x_x.txt):

Columns: timestamp (ns), x, y, z (rad/s)
Represents the rate of rotation around the x, y, and z axesâ€‹
â€‹
â€‹
.
Orientation Data (BSC_ori_x_x.txt):

Columns: timestamp (ns), Azimuth, Pitch, Roll (degrees)
Represents the phone's orientation in spaceâ€‹
"""

from google.colab import files
uploaded = files.upload()

import os
print(os.listdir("/content/"))  # List all files in the working directory

print(uploaded.keys())  # Lists all uploaded file names

import pandas as pd

# Load data with correct paths
acc_data = pd.read_csv("/content/BSC_acc_10_1.txt", comment='#', header=None, names=['timestamp', 'x_acc', 'y_acc', 'z_acc'])
gyro_data = pd.read_csv("/content/BSC_gyro_10_1.txt", comment='#', header=None, names=['timestamp', 'x_gyro', 'y_gyro', 'z_gyro'])
ori_data = pd.read_csv("/content/BSC_ori_10_1.txt", comment='#', header=None, names=['timestamp', 'azimuth', 'pitch', 'roll'])

print(acc_data.head())  # Verify if data loads correctly

"""Load and Explore Data in Python"""

import pandas as pd

# File paths
acc_file = "/content/BSC_acc_10_1.txt"
gyro_file = "/content/BSC_gyro_10_1.txt"
ori_file = "/content/BSC_ori_10_1.txt"

# Function to load sensor data while skipping metadata
def load_sensor_data(file_path, column_names):
    return pd.read_csv(file_path, comment='#', header=None, names=column_names, skiprows=1)

# Load data properly
acc_data = load_sensor_data(acc_file, ['timestamp', 'x_acc', 'y_acc', 'z_acc'])
gyro_data = load_sensor_data(gyro_file, ['timestamp', 'x_gyro', 'y_gyro', 'z_gyro'])
ori_data = load_sensor_data(ori_file, ['timestamp', 'azimuth', 'pitch', 'roll'])

# Ensure 'timestamp' is numeric before converting
acc_data = acc_data[pd.to_numeric(acc_data['timestamp'], errors='coerce').notna()]
gyro_data = gyro_data[pd.to_numeric(gyro_data['timestamp'], errors='coerce').notna()]
ori_data = ori_data[pd.to_numeric(ori_data['timestamp'], errors='coerce').notna()]

# Convert timestamp to datetime
acc_data['timestamp'] = pd.to_datetime(acc_data['timestamp'].astype(int), unit='ns')
gyro_data['timestamp'] = pd.to_datetime(gyro_data['timestamp'].astype(int), unit='ns')
ori_data['timestamp'] = pd.to_datetime(ori_data['timestamp'].astype(int), unit='ns')

# Print the first few rows
print(acc_data.head())
print(gyro_data.head())
print(ori_data.head())

"""Synchronizing the Data

Since data from different sensors have slightly different timestamps, use interpolation or resampling to align them.
"""

# Merge datasets based on timestamp (nearest match)
merged_data = pd.merge_asof(acc_data, gyro_data, on="timestamp")
merged_data = pd.merge_asof(merged_data, ori_data, on="timestamp")

print(merged_data.head())

"""Labeling the Data

Each file contains an Activity Label (e.g., 12 - Back Sitting Chair). You need to map these labels to their respective motion categories (e.g., Normal or Distress).
"""

# Define activity mapping
activity_mapping = {12: "Normal"}  # Extend for distress movements

# Assign labels
merged_data["activity"] = "Normal"  # Modify based on activity type

"""Feature Extraction

For machine learning, calculate relevant motion features:

Mean, Variance, Peak Values
Fast Fourier Transform (FFT) for Frequency Analysis
Standard Deviation, RMS Energy
"""

features = merged_data[['x_acc', 'y_acc', 'z_acc', 'x_gyro', 'y_gyro', 'z_gyro']].agg(['mean', 'std', 'max', 'min'])
print(features)

"""Visualizing Data

Use Matplotlib to visualize patterns:

"""

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
plt.plot(merged_data['timestamp'], merged_data['x_acc'], label='X Acceleration')
plt.plot(merged_data['timestamp'], merged_data['y_acc'], label='Y Acceleration')
plt.plot(merged_data['timestamp'], merged_data['z_acc'], label='Z Acceleration')
plt.legend()
plt.show()

""" Normalize Data Using MinMaxScaler

Since LSTMs work best with normalized data, scale everything between 0 and 1:
"""

from sklearn.preprocessing import MinMaxScaler
import numpy as np

# Select only feature columns (excluding timestamps and labels)
feature_columns = ['x_acc', 'y_acc', 'z_acc', 'x_gyro', 'y_gyro', 'z_gyro']  # Add extracted features

scaler = MinMaxScaler()
scaled_features = scaler.fit_transform(merged_data[feature_columns])  # Assuming merged_data is your final dataset

# Convert back to DataFrame
normalized_data = pd.DataFrame(scaled_features, columns=feature_columns)
normalized_data['timestamp'] = merged_data['timestamp']
normalized_data['activity'] = merged_data['activity']  # Keep labels

"""Create Sliding Windows (100 samples per window)

For time-series models like LSTM, we need to create overlapping sequences of 100 samples per window.
"""

def create_sliding_windows(data, window_size=100):
    sequences = []
    labels = []
    for i in range(len(data) - window_size):
        sequences.append(data.iloc[i:i+window_size, :-2].values)  # Exclude timestamp & activity
        labels.append(data.iloc[i+window_size-1, -1])  # Use last label in window

    return np.array(sequences), np.array(labels)

# Create windows
X, y = create_sliding_windows(normalized_data, window_size=100)

"""Train-Test Split (70%-30%)"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

"""Train LSTM Autoencoder for Normal Motion Detection

LSTM Autoencoder is trained only on normal motion to detect anomalies.
"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed

# Define model
model = Sequential([
    LSTM(64, activation='relu', input_shape=(100, X_train.shape[2]), return_sequences=True),
    LSTM(32, activation='relu', return_sequences=False),
    RepeatVector(100),
    LSTM(32, activation='relu', return_sequences=True),
    LSTM(64, activation='relu', return_sequences=True),
    TimeDistributed(Dense(X_train.shape[2]))
])

model.compile(optimizer='adam', loss='mse')
model.summary()

# Train
history = model.fit(X_train, X_train, epochs=50, batch_size=32, validation_split=0.1)

"""Train One-Class SVM for Anomaly Detection
We use only normal data to train One-Class SVM.
"""

from sklearn.svm import OneClassSVM

# Flatten data for One-Class SVM
X_train_flatten = X_train.reshape(X_train.shape[0], -1)
X_test_flatten = X_test.reshape(X_test.shape[0], -1)

# Train One-Class SVM
svm = OneClassSVM(kernel="rbf", gamma="auto").fit(X_train_flatten)

# Predict anomalies
y_pred = svm.predict(X_test_flatten)
y_pred = [1 if x == 1 else -1 for x in y_pred]  # Convert to anomaly labels

"""Convert LSTM Model to TensorFlow Lite
For mobile deployment, convert the trained model to TensorFlow Lite:
"""

converter = tf.lite.TFLiteConverter.from_keras_model(model)

# Enable resource variables (fixes TensorListReserve error)
converter.experimental_enable_resource_variables = True

# Allow TensorFlow ops (fixes TensorArray issues)
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,  # Default TFLite ops
    tf.lite.OpsSet.SELECT_TF_OPS     # Allow some TensorFlow ops
]

# Disable lowering of tensor list ops
converter._experimental_lower_tensor_list_ops = False

# Convert the model
tflite_model = converter.convert()

# Save the converted model
with open("lstm_autoencoder.tflite", "wb") as f:
    f.write(tflite_model)

print("TFLite model successfully converted!")

"""## Automate Hyperparameter Search with KerasTuner

Use KerasTuner to find the best combination automatically.
"""

!pip install keras-tuner

"""## Hyperparameter Tuning for LSTM Autoencoder

What this does:

Tests different LSTM units, dropout rates, and learning rates
Finds the best combination automatically
Retrains the model using the best settings

Key Hyperparameters to Tune
Number of LSTM Layers & Units

Too many units â†’ Overfitting
Too few units â†’ Poor learning
Common choices: [32, 64, 128]
Batch Size

Small batch (16, 32) â†’ Better generalization
Large batch (64, 128) â†’ Faster training
Learning Rate (0.001 - 0.0001)

Too high â†’ Won't converge
Too low â†’ Slow training
Dropout (0.1 - 0.5)

Prevents overfitting in LSTM layers
"""

import keras_tuner as kt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed, Dropout
import tensorflow as tf

# Define the model function
def build_model(hp):
    model = Sequential([
        LSTM(hp.Int("units1", min_value=32, max_value=128, step=32), activation='relu', return_sequences=True, input_shape=(100, X_train.shape[2])),
        Dropout(hp.Float("dropout1", min_value=0.1, max_value=0.5, step=0.1)),
        LSTM(hp.Int("units2", min_value=32, max_value=128, step=32), activation='relu', return_sequences=False),
        RepeatVector(100),
        LSTM(hp.Int("units3", min_value=32, max_value=128, step=32), activation='relu', return_sequences=True),
        Dropout(hp.Float("dropout2", min_value=0.1, max_value=0.5, step=0.1)),
        LSTM(hp.Int("units4", min_value=32, max_value=128, step=32), activation='relu', return_sequences=True),
        TimeDistributed(Dense(X_train.shape[2]))
    ])

    model.compile(optimizer=tf.keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-3, 1e-4])),
                  loss='mse')

    return model

# Define tuner
tuner = kt.RandomSearch(
    build_model,
    objective='val_loss',
    max_trials=10,  # Number of different combinations to try
    executions_per_trial=2,  # Number of times to train each model
    directory='tuning_results',
    project_name='LSTM_tuning'
)

# Start tuning
tuner.search(X_train, X_train, epochs=20, validation_split=0.1, batch_size=32)

# Get best hyperparameters
best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]
print(f"Best Hyperparameters: {best_hps.values}")

# Train model with best hyperparameters
best_model = tuner.hypermodel.build(best_hps)
best_model.fit(X_train, X_train, epochs=50, validation_split=0.1, batch_size=32)

"""## MODEL EVALUATION

Evaluate LSTM Autoencoder

Since this is an anomaly detection model, evaluate using Reconstruction Error.
"""

import numpy as np

# Predict on test data
X_pred = best_model.predict(X_test)

# Compute Mean Squared Error (MSE)
mse = np.mean(np.power(X_test - X_pred, 2), axis=(1, 2))

# Plot reconstruction error
import matplotlib.pyplot as plt
plt.figure(figsize=(10,5))
plt.hist(mse, bins=50)
plt.xlabel("Reconstruction Error")
plt.ylabel("Count")
plt.title("LSTM Autoencoder Reconstruction Error")
plt.show()

# Set anomaly threshold
threshold = np.percentile(mse, 95)  # Set at 95th percentile

# Detect anomalies
anomalies = mse > threshold
print(f"Anomaly Count: {sum(anomalies)}")

"""## Hyperparameter Tuning for One-Class SVM

For One-Class SVM, tune the following:

Kernel Type (rbf, linear, poly)
nu (Anomaly Ratio)

Typical range: 0.01 - 0.1
Gamma (auto or scale)

auto: Uses 1/n_features
scale: Uses 1 / (n_features * X.var())

What this does:

Tests multiple kernel types, nu, and gamma values
Finds the best SVM configuration
Retrains the model with best settings
"""

from sklearn.model_selection import GridSearchCV
from sklearn.svm import OneClassSVM
from sklearn.metrics import make_scorer
import numpy as np

# Define parameter grid
param_grid = {
    'kernel': ['rbf', 'linear', 'poly'],
    'nu': [0.01, 0.05, 0.1],
    'gamma': ['auto', 'scale']
}

# Custom scoring function for One-Class SVM
def anomaly_scorer(estimator, X):
    scores = estimator.decision_function(X)  # Higher values mean normal, lower means anomaly
    return np.mean(scores)  # Maximize the mean decision score

# Create a scorer for GridSearchCV
scorer = make_scorer(anomaly_scorer, greater_is_better=True)

# One-Class SVM model
svm = OneClassSVM()

# Apply GridSearchCV with custom scoring
grid_search = GridSearchCV(svm, param_grid, cv=3, scoring=scorer, verbose=2)

# Fit on training data
grid_search.fit(X_train_flatten)

# Print best hyperparameters
print("Best SVM Hyperparameters:", grid_search.best_params_)

# Train the best model
best_svm = OneClassSVM(**grid_search.best_params_)
best_svm.fit(X_train_flatten)

# Train the final One-Class SVM model with best parameters
best_svm = OneClassSVM(gamma='auto', nu=0.01)
best_svm.fit(X_train_flatten)

# Save the trained model
import joblib
joblib.dump(best_svm, "best_one_class_svm.pkl")

print("âœ… Best One-Class SVM model saved successfully!")

# Predict anomalies on test data
y_pred = best_svm.predict(X_test_flatten)

# Convert predictions (-1 = anomaly, 1 = normal)
y_pred_binary = [1 if x == 1 else 0 for x in y_pred]

# Print the number of anomalies detected
print(f"ðŸš¨ Anomalies Detected: {sum(np.array(y_pred_binary) == 0)}")

"""## EvaluateD  One-Class SVM with Classification Report and Accuracy"""

from sklearn.metrics import classification_report, accuracy_score
import numpy as np

# Ensure y_test is numerical (convert string labels if needed)
# Assuming normal is "Normal" and anomaly is "Anomaly" (update based on your dataset)
y_test_numeric = np.where(np.array(y_test) == "Normal", 1, 0)  # Convert strings to 1 (normal) and 0 (anomaly)

# Predict anomalies (-1 means anomaly, 1 means normal)
y_pred = best_svm.predict(X_test_flatten)

# Convert predictions to binary (1=normal, 0=anomaly)
y_pred_binary = np.where(y_pred == 1, 1, 0)

# Ensure y_test and y_pred_binary have the same type and shape
y_test_numeric = y_test_numeric.astype(int)
y_pred_binary = y_pred_binary.astype(int)

# Evaluate the model
print("âœ… SVM Classification Report:")
print(classification_report(y_test_numeric, y_pred_binary))
print(f"âœ… Accuracy: {accuracy_score(y_test_numeric, y_pred_binary) * 100:.2f}%")

from google.colab import files
files.download("lstm_autoencoder.tflite")

"""## Mobile App Backend Architecture

Key Components

Mobile Sensors: Get real-time accelerometer & gyroscope data.


Edge AI Model (TensorFlow Lite/PyTorch Mobile): Run on-device AI detection.

Backend Server (Flask/FastAPI): Process real-time alerts.

SOS Triggering: Send distress signals via SMS, WhatsApp, or Call.
"""